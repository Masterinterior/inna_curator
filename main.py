import os
import re
import time
import json
import base64
import asyncio
import math
from typing import Dict, List, Any, Optional, Tuple

import requests
from fastapi import FastAPI, Request

# ================= ENV =================
TG_BOT_TOKEN = os.getenv("TG_BOT_TOKEN", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

HTTP_TIMEOUT = 25
MAX_IMAGE_BYTES = 6_000_000

# Knowledge base files
KB_PATH = "knowledge/knowledge.txt"
KB_EMB_PATH = "knowledge/embeddings.json"  # prebuilt semantic index

# ================= APP =================
app = FastAPI()

# ================= DAILY MESSAGE LIMIT =================
DAILY_LIMIT = 70  # –æ—Ç–≤–µ—Ç–æ–≤ –ò–Ω–Ω—ã –≤ —Å—É—Ç–∫–∏ –Ω–∞ –æ–¥–∏–Ω —á–∞—Ç
DAILY_COUNTER: Dict[int, Dict[str, int]] = {}

def _today_key() -> str:
    return time.strftime("%Y-%m-%d", time.localtime())

def can_reply_today(chat_id: int) -> Tuple[bool, int]:
    day = _today_key()
    rec = DAILY_COUNTER.get(chat_id)
    if not rec or rec.get("day") != day:
        DAILY_COUNTER[chat_id] = {"day": day, "count": 0}
        rec = DAILY_COUNTER[chat_id]
    remaining = max(0, DAILY_LIMIT - int(rec.get("count", 0)))
    return (remaining > 0, remaining)

def inc_today(chat_id: int):
    day = _today_key()
    rec = DAILY_COUNTER.get(chat_id)
    if not rec or rec.get("day") != day:
        DAILY_COUNTER[chat_id] = {"day": day, "count": 0}
        rec = DAILY_COUNTER[chat_id]
    rec["count"] = int(rec.get("count", 0)) + 1

# ================= MEMORY =================
CONTEXT_LIMIT = 14  # keep last 12-14 messages
CHAT_CONTEXT: Dict[int, List[Dict[str, Any]]] = {}

# recent assistant outputs (to reduce repetition)
RECENT_ASSISTANT: Dict[int, List[str]] = {}
RECENT_LIMIT = 6

# ======== IMAGE HISTORY ========
IMAGE_KEEP = 12  # store more now because albums
IMAGE_HISTORY: Dict[int, List[Dict[str, Any]]] = {}
IMAGE_SEQ: Dict[int, int] = {}  # photo counter: #1, #2, ...

# ======== ALBUM BUFFER (Variant 2) ========
ALBUM_DEBOUNCE_SEC = 2.0
ALBUM_BUFFER: Dict[Tuple[int, str], Dict[str, Any]] = {}  # (chat_id, album_id) -> data

# legacy last image store (optional)
LAST_IMAGE: Dict[int, bytes] = {}
LAST_IMAGE_AT: Dict[int, float] = {}
IMAGE_TTL = 60 * 30  # 30 minutes

def has_fresh_image(chat_id: int) -> bool:
    if chat_id not in LAST_IMAGE:
        return False
    return (time.time() - LAST_IMAGE_AT.get(chat_id, 0)) <= IMAGE_TTL

# ================= SYSTEM ROLE =================
SYSTEM_ROLE = (
    "–¢—ã ‚Äî –ò–Ω–Ω–∞, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞ –∏ –∫—É—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —à–∫–æ–ª—ã. "
    "–£ —Ç–µ–±—è –µ—Å—Ç—å –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ —É—Ä–æ–∫–∞–º –∏ —Ç—ã –≤—Å–µ–≥–¥–∞ –æ–ø–∏—Ä–∞–µ—à—å—Å—è –Ω–∞ –Ω–µ—ë, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ. "
    "–¢—ã —É–≤–µ—Ä–µ–Ω–Ω–∞—è, —ç–º–ø–∞—Ç–∏—á–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞. "
    "–¢—ã –≥–æ–≤–æ—Ä–∏—à—å —Å–ø–æ–∫–æ–π–Ω–æ, –ø–æ –¥–µ–ª—É, –±–µ–∑ –∑–∞—É–º–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –Ω–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ. "
    "–í–ê–ñ–ù–û: –ü–∏—à–∏ –±–µ–∑ Markdown (**–∑–≤—ë–∑–¥–æ—á–µ–∫**). –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—ã–¥–µ–ª–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π HTML-—Ç–µ–≥–∏ <b> –∏ <i>. "
    "–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å—ã–ª–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –≤–æ–ø—Ä–æ—Å–∞ ‚Äî "
    "—Ç—ã –¥–∞—ë—à—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç—ë–ø–ª—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), "
    "–æ—Ç–º–µ—á–∞–µ—à—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã "
    "–∏ –º—è–≥–∫–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å, —á–µ–º –º–æ–∂–µ—à—å –ø–æ–º–æ—á—å –¥–∞–ª—å—à–µ. "
    "–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å ‚Äî –æ—Ç–≤–µ—á–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ. "
    "–ï—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—à—å –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é ‚Äî –≤—Å–µ–≥–¥–∞ –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
    "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ –∏ —É—Ä–æ–∫–∏ ‚Äî –ù–ï –ø–∏—à–∏ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã ¬´–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫—É—Ä—Å–∞¬ª –∏ —Ç.–ø., "
    "–∞ —Å—Ä–∞–∑—É –¥–∞–≤–∞–π —Ç–æ—á–Ω—ã–µ –º–µ—Å—Ç–∞ –∏–∑ –Ω–∞—à–µ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã. "
    "–ß–∞—Å—Ç–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–π –æ—Ç–≤–µ—Ç –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å: ¬´–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É‚Ä¶¬ª. "
    "–ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∂–∏ —É–º–µ—Å—Ç–Ω–æ."
)

# ================= TOPIC GUARD (FORBIDDEN TOPICS) =================
ALLOWED_TOPIC_RE = re.compile(
    r"(–¥–∏–∑–∞–π–Ω|–∏–Ω—Ç–µ—Ä—å–µ—Ä|—Ä–µ–º–æ–Ω—Ç|–æ—Ç–¥–µ–ª–∫|–ø–ª–∞–Ω–∏—Ä–æ–≤–∫|–ø–µ—Ä–µ–ø–ª–∞–Ω–∏—Ä–æ–≤–∫|–∑–æ–Ω–∏—Ä–æ–≤–∞–Ω|—ç—Ä–≥–æ–Ω–æ–º–∏–∫|"
    r"–º–µ–±–µ–ª|–∫—É—Ö–Ω|–≤–∞–Ω–Ω|—Å–∞–Ω—É–∑|—Å–ø–∞–ª—å–Ω|–≥–æ—Å—Ç–∏–Ω|–¥–µ—Ç—Å–∫|–ø—Ä–∏—Ö–æ–∂|"
    r"—Å–≤–µ—Ç|–æ—Å–≤–µ—â–µ–Ω|—ç–ª–µ–∫—Ç—Ä–∏–∫|—Å–∞–Ω—Ç–µ—Ö|–ø—Ä–æ—Ä–∞–±|–ø–æ–¥—Ä—è–¥—á–∏–∫|—Å—Ç—Ä–æ–∏—Ç–µ–ª|"
    r"–º–∞—Ç–µ—Ä–∏–∞–ª|—Ñ–∞–∫—Ç—É—Ä|–ø–ª–∏—Ç–∫|–∫—Ä–∞—Å–∫|–æ–±–æ–∏|–ø–∞—Ä–∫–µ—Ç|–ª–∞–º–∏–Ω–∞—Ç|"
    r"—Å—Ç–∏–ª|–º–∏–¥|mid|–º–µ–º—Ñ–∏—Å|memphis|–ª–æ—Ñ—Ç|—Å–∫–∞–Ω–¥–∏|–¥–∂–∞–ø–∞–Ω–¥–∏|"
    r"–æ–±—É—á–µ–Ω|–∫—É—Ä—Å|—É—Ä–æ–∫|–º–æ–¥—É–ª|—Å—Ç—É–ø–µ–Ω|–¥–∑|–¥–æ–º–∞—à–Ω|"
    r"homestyler|remplanner|archicad|3ds|max|photoshop|ps|canva|figma|"
    r"–Ω–µ–π—Ä–æ—Å–µ—Ç|ai|–∏–∏|midjourney|stable|prompt|–ø—Ä–æ–º—Ç|—Ä–µ–Ω–¥–µ—Ä|–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü|"
    r"—Å–æ—Ü—Å–µ—Ç|–∫–æ–Ω—Ç–µ–Ω—Ç|—Å—Ç–æ—Ä–∏—Å|—Ä–∏–ª—Å|—é—Ç—É–±|–≤–∫|—Ç–µ–ª–µ–≥—Ä–∞–º|–ø—Ä–æ–¥–≤–∏–∂–µ–Ω|–ª–∏—á–Ω(—ã–π|–æ–≥–æ)\s+–±—Ä–µ–Ω–¥|"
    r"—Ü–µ–Ω(–∞|—ã)|–ø—Ä–∞–π—Å|—Å—Ç–æ–∏–º–æ—Å—Ç|–∫–æ–º–º–µ—Ä—á–µ—Å–∫|–∫–ø|–ø—Ä–æ–¥–∞–∂|–∫–ª–∏–µ–Ω—Ç|"
    r"–¥–æ–≥–æ–≤–æ—Ä|–æ—Ñ–µ—Ä—Ç|—Å—á–µ—Ç|–∞–∫—Ç|–ø—Ä–µ–¥–æ–ø–ª–∞—Ç|–æ–ø–ª–∞—Ç|–∞–≤–∞–Ω—Å|–ø—Ä–∞–≤(–æ|–∞)|—é—Ä–∏–¥–∏—á–µ—Å–∫|–∏–ø|–æ–æ–æ|—É—Å–Ω|–Ω–¥—Å)",
    re.IGNORECASE,
)

FORBIDDEN_TOPIC_RE = re.compile(
    r"(—Ñ–∏–∑–∏–∫|–∫–≤–∞–Ω—Ç|–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª|—Ñ–æ—Ä–º—É–ª|–∏–Ω—Ç–µ–≥—Ä–∞–ª|–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü|–º–∞—Ç–µ–º–∞—Ç|"
    r"–º–µ–¥–∏—Ü–∏–Ω|–±–æ–ª–µ–∑–Ω|—Å–∏–º–ø—Ç–æ–º|–¥–∏–∞–≥–Ω–æ–∑|—Ç–∞–±–ª–µ—Ç–∫|–ª–µ–∫–∞—Ä—Å—Ç–≤|–∞–Ω–∞–ª–∏–∑(—ã)?|"
    r"–ø–æ–ª–∏—Ç–∏–∫|–≤—ã–±–æ—Ä|–ø–∞—Ä—Ç(–∏—è|–∏–∏)|—Å–∞–Ω–∫—Ü|"
    r"–∏–Ω–≤–µ—Å—Ç–∏—Ü|–∞–∫—Ü–∏|–ø–æ—Ä—Ç—Ñ–µ–ª|–æ–±–ª–∏–≥–∞—Ü|–∫—Ä–∏–ø—Ç|–±–∏—Ç–∫–æ–∏–Ω|–∫—É—Ä—Å\s+–≤–∞–ª—é—Ç|"
    r"—ç–∑–æ—Ç–µ—Ä–∏–∫|–∞—Å—Ç—Ä–æ–ª|—Ç–∞—Ä–æ|"
    r"–ø–æ–¥–∞—Ä(–æ–∫|–∫–∏)|—á—Ç–æ\s+–ø–æ–¥–∞—Ä–∏—Ç—å|–∏–¥–µ–∏\s+–ø–æ–¥–∞—Ä–∫–æ–≤?|"
    r"–¥–µ—Ç(—è–º|–µ–π)|—Ä–µ–±–µ–Ω(–æ–∫|–∫–∞|–∫—É)|—à–∫–æ–ª|—Å–∞–¥–∏–∫|–∏–≥—Ä—É—à–∫|"
    r"–ø—Ä–∞–∑–¥–Ω–∏–∫|–¥–µ–Ω—å\s+—Ä–æ–∂–¥–µ–Ω–∏—è|–Ω(–æ–≤—ã–π|–≥–æ–¥–∞)|—Ä–æ–∂–¥–µ—Å—Ç–≤|8\s+–º–∞—Ä—Ç–∞|23\s+—Ñ–µ–≤—Ä–∞–ª—è|"
    r"–±—ã—Ç|–ª–∞–π—Ñ—Å—Ç–∞–π–ª|—Ä–µ—Ü–µ–ø—Ç|–µ–¥–∞|–≥–æ—Ç–æ–≤(–∏—Ç—å|–∫–∞)|"
    r"–æ—Ç–Ω–æ—à–µ–Ω(–∏—è|–∏–π)|–º—É–∂|–∂–µ–Ω–∞|–¥–µ–≤—É—à–∫|–ø–∞—Ä–µ–Ω—å|–ª—é–±–æ–≤—å|"
    r"—Ä–∞—Å—Å—Ç–∞—Ç|—Ä–∞–∑–≤–æ–¥|–∏–∑–º–µ–Ω(–∞|—ã)|—Ä–µ–≤–Ω–æ—Å—Ç|"
    r"–ø—Å–∏—Ö–æ–ª–æ–≥|–ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø|—á—É–≤—Å—Ç–≤|–æ–±–∏–¥|—Ç–æ–∫—Å–∏—á–Ω|"
    r"18\+|–ø–æ—Ä–Ω–æ|—ç—Ä–æ—Ç–∏–∫)",
    re.IGNORECASE,
)

OFFTOP_REPLY = (
    "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –ø–æ –¥–∏–∑–∞–π–Ω—É –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞ –∏ —Ä–µ–º–æ–Ω—Ç—É, –æ–±—É—á–µ–Ω–∏—é –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –¥–∏–∑–∞–π–Ω–µ—Ä–∞, "
    "–¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–æ–º—É —Å–æ—Ñ—Ç—É, AI –¥–ª—è –¥–∏–∑–∞–π–Ω–∞, –ª–∏—á–Ω–æ–º—É –±—Ä–µ–Ω–¥—É/–∫–æ–Ω—Ç–µ–Ω—Ç—É, —Ü–µ–Ω–∞–º –∏ –¥–æ–≥–æ–≤–æ—Ä–∞–º. "
    "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –≤ —ç—Ç–∏—Ö —Ä–∞–º–∫–∞—Ö ‚Äî –∏ —è –ø–æ–º–æ–≥—É üôÇ"
)

def is_forbidden_topic(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    if FORBIDDEN_TOPIC_RE.search(t) and not ALLOWED_TOPIC_RE.search(t):
        return True
    return False

# ================= TELEGRAM TEXT SANITIZE =================
MD_BOLD_RE = re.compile(r"\*\*(.+?)\*\*")
MD_ITALIC_RE = re.compile(r"(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)")

def to_tg_html(text: str) -> str:
    """
    Telegram uses parse_mode=HTML.
    Convert basic Markdown (**bold**, *italic*) to HTML to avoid showing stars.
    Keeps existing HTML tags (KB already uses <b>).
    """
    t = (text or "").strip()
    if not t:
        return t
    t = MD_BOLD_RE.sub(r"<b>\1</b>", t)
    t = MD_ITALIC_RE.sub(r"<i>\1</i>", t)
    t = t.replace("**", "")
    return t

# ================= TELEGRAM =================
def tg_send(chat_id: int, text: str):
    try:
        safe = to_tg_html(text)
        r = requests.post(
            f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage",
            json={
                "chat_id": chat_id,
                "text": safe,
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            },
            timeout=HTTP_TIMEOUT,
        )
        if r.status_code != 200:
            print("TG sendMessage error:", r.status_code, r.text)
    except Exception as e:
        print("TG sendMessage exception:", repr(e))

def tg_typing(chat_id: int):
    try:
        requests.post(
            f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendChatAction",
            json={"chat_id": chat_id, "action": "typing"},
            timeout=HTTP_TIMEOUT,
        )
    except Exception as e:
        print("TG typing exception:", repr(e))

def tg_get_photo(file_id: str) -> Optional[bytes]:
    try:
        meta = requests.get(
            f"https://api.telegram.org/bot{TG_BOT_TOKEN}/getFile",
            params={"file_id": file_id},
            timeout=HTTP_TIMEOUT,
        ).json()

        if not meta.get("ok"):
            return None

        path = meta["result"]["file_path"]
        img = requests.get(
            f"https://api.telegram.org/file/bot{TG_BOT_TOKEN}/{path}",
            timeout=HTTP_TIMEOUT,
        ).content

        if img and len(img) <= MAX_IMAGE_BYTES:
            return img
        return None
    except Exception as e:
        print("TG getPhoto exception:", repr(e))
        return None

# ================= OPENAI =================
def openai_chat(messages: List[Dict[str, Any]], max_tokens: int = 900, temperature: float = 0.45) -> str:
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)

        r = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (r.choices[0].message.content or "").strip()
    except Exception as e:
        print("OPENAI chat exception:", repr(e))
        return "–°–µ–π—á–∞—Å —É –º–µ–Ω—è –Ω–µ–±–æ–ª—å—à–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–∞—É–∑–∞ üôè –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É."

def openai_with_image(
    prompt: str,
    image: bytes,
    context: List[Dict[str, Any]],
    max_tokens: int = 900,
    temperature: float = 0.55,
) -> str:
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)

        b64 = base64.b64encode(image).decode()

        messages = (
            [{"role": "system", "content": SYSTEM_ROLE}]
            + context
            + [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}},
                ],
            }]
        )

        r = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (r.choices[0].message.content or "").strip()
    except Exception as e:
        print("OPENAI image exception:", repr(e))
        return "–Ø –≤–∏–∂—É, —á—Ç–æ –ø—Ä–∏—à–ª–æ —Ñ–æ—Ç–æ, –Ω–æ —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –µ–≥–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –∏–∑-–∑–∞ —Ç–µ—Ö. –ø–∞—É–∑—ã üôè –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑."

def openai_embed(text: str) -> Optional[List[float]]:
    """
    Creates embedding for query (fast & cheap).
    """
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)
        r = client.embeddings.create(
            model="text-embedding-3-small",
            input=[text],
        )
        return r.data[0].embedding
    except Exception as e:
        print("OPENAI embed exception:", repr(e))
        return None

# ================= CONTEXT =================
def add_context(chat_id: int, role: str, content: str):
    CHAT_CONTEXT.setdefault(chat_id, [])
    CHAT_CONTEXT[chat_id].append({"role": role, "content": content})
    CHAT_CONTEXT[chat_id] = CHAT_CONTEXT[chat_id][-CONTEXT_LIMIT:]

def remember_assistant(chat_id: int, text: str):
    RECENT_ASSISTANT.setdefault(chat_id, [])
    RECENT_ASSISTANT[chat_id].append((text or "")[:900])
    RECENT_ASSISTANT[chat_id] = RECENT_ASSISTANT[chat_id][-RECENT_LIMIT:]

def avoid_repetition_hint(chat_id: int) -> str:
    recent = RECENT_ASSISTANT.get(chat_id) or []
    if not recent:
        return ""
    last = "\n---\n".join(recent[-3:])
    return (
        "–í–ê–ñ–ù–û: –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π –¥–æ—Å–ª–æ–≤–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã –∏ –∏–∑–±–µ–≥–∞–π —à—Ç–∞–º–ø–æ–≤ "
        "(¬´—É—é—Ç–Ω–æ –∏ —Å—Ç–∏–ª—å–Ω–æ¬ª, ¬´—Ç–µ–ø–ª–∞—è –ø–∞–ª–∏—Ç—Ä–∞¬ª, ¬´–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ—Å—Ç—å¬ª, ¬´–µ—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É‚Ä¶¬ª). "
        "–ú–µ–Ω—è–π –ª–µ–∫—Å–∏–∫—É –∏ —Ñ–æ–∫—É—Å: –∫–æ–º–ø–æ–∑–∏—Ü–∏—è/—Å–≤–µ—Ç/—Ü–≤–µ—Ç/—Ñ—É–Ω–∫—Ü–∏—è/–º–∞—Ç–µ—Ä–∏–∞–ª—ã.\n"
        f"–ù–ï –ü–û–í–¢–û–†–Ø–ô —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤:\n{last}"
    )

# ======== IMAGE HISTORY HELPERS ========
IMAGE_REF_RE = re.compile(
    r"(–Ω–∞\s+—Ñ–æ—Ç–æ|–Ω–∞\s+–∫–∞—Ä—Ç–∏–Ω–∫–µ|–ø–æ\s+—Ñ–æ—Ç–æ|–ø–æ\s+–∫–∞—Ä—Ç–∏–Ω–∫–µ|–ø–æ—Å–º–æ—Ç—Ä–∏|–æ—Ü–µ–Ω(–∏|–∫–∞)|"
    r"—á—Ç–æ\s+–Ω–µ\s+—Ç–∞–∫|—á—Ç–æ\s+–∏—Å–ø—Ä–∞–≤–∏—Ç—å|–ø–µ—Ä–µ–¥–µ–ª–∞–π|–≤–∞—Ä–∏–∞–Ω—Ç|–ø–ª–∞–Ω–∏—Ä–æ–≤–∫|"
    r"–≤\s+—ç—Ç–æ–º\s+–∏–Ω—Ç–µ—Ä—å–µ—Ä–µ|–∑–¥–µ—Å—å|—Ç—É—Ç|—ç—Ç–æ\s+—Ñ–æ—Ç–æ|—ç—Ç–æ\s+–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ|"
    r"–ø—Ä–µ–¥—ã–¥—É—â(–µ–µ|–µ–º)\s+—Ñ–æ—Ç–æ|–ø—Ä–æ—à–ª(–æ–µ|–æ–º)\s+—Ñ–æ—Ç–æ|–ø–µ—Ä–≤(–æ–µ|–æ–º)\s+—Ñ–æ—Ç–æ|–≤—Ç–æ—Ä(–æ–µ|–æ–º)\s+—Ñ–æ—Ç–æ|—Ç—Ä–µ—Ç—å(–µ|–µ–º)\s+—Ñ–æ—Ç–æ)",
    re.IGNORECASE,
)
ORDINAL_RE = re.compile(r"\b(–ø–µ—Ä–≤|–≤—Ç–æ—Ä|—Ç—Ä–µ—Ç|—á–µ—Ç–≤–µ—Ä|–ø—è—Ç)\w*\b", re.IGNORECASE)

VISUAL_TOPIC_RE = re.compile(
    r"(–∏–Ω—Ç–µ—Ä—å–µ—Ä|–∫–æ–º–Ω–∞—Ç|–ø–æ–º–µ—â–µ–Ω|–ø–ª–∞–Ω–∏—Ä–æ–≤–∫|—Å—Ç–∏–ª|—Ü–≤–µ—Ç|–ø–∞–ª–∏—Ç—Ä|—Å–≤–µ—Ç|–æ—Å–≤–µ—â–µ–Ω|"
    r"–¥–∏–≤–∞–Ω|–∫—Ä–µ—Å–ª|–∫—É—Ö–Ω|–≤–∞–Ω–Ω|—Å–∞–Ω—É–∑|—Å–ø–∞–ª—å–Ω|–≥–æ—Å—Ç–∏–Ω|–¥–µ—Ç—Å–∫|–ø—Ä–∏—Ö–æ–∂|"
    r"—Å–ª–∏—à–∫–æ–º|–º—É–∂—Å–∫|–∂–µ–Ω—Å–∫|–¥–µ–≤—á–∞—á|—É—é—Ç–Ω|—Ö–æ–ª–æ–¥–Ω|—Ç–µ–ø–ª|–¥–µ—à–µ–≤|–¥–æ—Ä–æ–≥|"
    r"—á—Ç–æ\s+–¥–æ–±–∞–≤–∏—Ç—å|—á—Ç–æ\s+—É–±—Ä–∞—Ç—å|–∫–∞–∫\s+—É–ª—É—á—à–∏—Ç—å|–∫–∞–∫\s+–∏—Å–ø—Ä–∞–≤–∏—Ç—å)",
    re.IGNORECASE,
)

COMPARE_RE = re.compile(
    r"(–∫–∞–∫–æ–π\s+–≤–∞—Ä–∏–∞–Ω—Ç|—á—Ç–æ\s+–ª—É—á—à–µ|—Å—Ä–∞–≤–Ω–∏|–ª–µ–≤—ã–π|–ø—Ä–∞–≤—ã–π|1\s+–∏–ª–∏\s+2|–ø–µ—Ä–≤—ã–π\s+–∏–ª–∏\s+–≤—Ç–æ—Ä–æ–π|–≤—ã–±–µ—Ä–∏\s+–ª—É—á—à–∏–π|–∫–∞–∫–æ–π\s+–Ω—Ä–∞–≤–∏—Ç—Å—è)",
    re.IGNORECASE,
)
PHOTO_NUM_RE = re.compile(r"#\s*(\d+)")

def push_image(chat_id: int, img: bytes, desc: str, album_id: Optional[str] = None) -> int:
    IMAGE_SEQ[chat_id] = IMAGE_SEQ.get(chat_id, 0) + 1
    num = IMAGE_SEQ[chat_id]

    IMAGE_HISTORY.setdefault(chat_id, [])
    IMAGE_HISTORY[chat_id].append({
        "num": num,
        "ts": time.time(),
        "image": img,
        "desc": (desc or "").strip(),
        "album_id": album_id,
    })
    IMAGE_HISTORY[chat_id] = IMAGE_HISTORY[chat_id][-IMAGE_KEEP:]
    return num

def pick_image_from_history(chat_id: int, user_text: str) -> Optional[Dict[str, Any]]:
    hist = IMAGE_HISTORY.get(chat_id) or []
    if not hist:
        return None
    t = (user_text or "").lower()

    if "–ø—Ä–µ–¥—ã–¥—É—â" in t or "–ø—Ä–æ—à–ª" in t:
        return hist[-2] if len(hist) >= 2 else hist[-1]

    nums = [int(x) for x in PHOTO_NUM_RE.findall(user_text)]
    if nums:
        target = nums[0]
        for it in hist:
            if it.get("num") == target:
                return it

    if ORDINAL_RE.search(t):
        if "–ø–µ—Ä–≤" in t:
            target = 1
        elif "–≤—Ç–æ—Ä" in t:
            target = 2
        elif "—Ç—Ä–µ—Ç" in t:
            target = 3
        elif "—á–µ—Ç–≤–µ—Ä" in t:
            target = 4
        elif "–ø—è—Ç" in t:
            target = 5
        else:
            target = None
        if target is not None:
            for it in hist:
                if it.get("num") == target:
                    return it
            return hist[-1]

    return hist[-1]

def build_visual_context_messages(chat_id: int, limit: int = 4) -> List[Dict[str, Any]]:
    hist = IMAGE_HISTORY.get(chat_id) or []
    if not hist:
        return []
    tail = hist[-limit:]
    parts = []
    for it in tail:
        num = it.get("num", "?")
        desc = (it.get("desc") or "").strip()
        if desc:
            parts.append(f"–§–æ—Ç–æ #{num}: {desc}")
    if not parts:
        return []
    joined = "\n\n".join(parts).strip()
    return [{"role": "assistant", "content": f"–í–∏–∑—É–∞–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Ñ–æ—Ç–æ:\n{joined}"}]

def get_context(chat_id: int) -> List[Dict[str, Any]]:
    ctx = CHAT_CONTEXT.get(chat_id, []).copy()
    return build_visual_context_messages(chat_id, limit=4) + ctx

# ================= INTENT: KB LINKS VS HOW-TO =================
COURSE_LOCATOR_RE = re.compile(
    r"(–≤\s+–∫–∞–∫–æ–º\s+—É—Ä–æ–∫–µ|–∫–∞–∫–æ–π\s+—É—Ä–æ–∫|–≥–¥–µ\s+–≤\s+–∫—É—Ä—Å–µ|–≥–¥–µ\s+—ç—Ç–æ\s+–≤\s+–æ–±—É—á–µ–Ω–∏–∏|"
    r"–≥–¥–µ\s+–ø–æ—Å–º–æ—Ç—Ä–µ—Ç|–≤\s+–∫–∞–∫–æ–º\s+–º–æ–¥—É–ª|–≤\s+–∫–∞–∫–æ–π\s+—Å—Ç—É–ø–µ–Ω|"
    r"–ª–µ–∂–∏—Ç\s+–≤\s+–ø—Ä–æ–≥—Ä–∞–º–º–µ|–æ—Ç–∫—Ä—ã—Ç—å\s+—É—Ä–æ–∫|–Ω–∞–π—Ç–∏\s+—É—Ä–æ–∫)",
    re.IGNORECASE,
)

HOWTO_RE = re.compile(
    r"(–∫–∞–∫\s+—Å–¥–µ–ª–∞—Ç—å|–∫–∞–∫\s+—Å–æ–±—Ä–∞—Ç—å|–∫–∞–∫\s+—Å–æ–∑–¥–∞—Ç—å|–∫–∞–∫\s+–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å|"
    r"–æ–±—ä—è—Å–Ω–∏|–ø–æ–∫–∞–∂–∏|–∏–Ω—Å—Ç—Ä—É–∫—Ü|–ø–æ—à–∞–≥–æ–≤–æ|–º–Ω–µ\s+–Ω—É–∂–Ω–æ|—Ö–æ—á—É\s+—Å–¥–µ–ª–∞—Ç—å|"
    r"–Ω–µ\s+–ø–æ–Ω—è–ª|–Ω–µ\s+–ø–æ–Ω—è–ª–∞|–Ω–µ\s+–ø–æ–ª—É—á–∞–µ—Ç—Å—è|–æ—à–∏–±–∫–∞|–ø–æ—á–µ–º—É)",
    re.IGNORECASE,
)

LIST_LESSONS_RE = re.compile(
    r"(–ø–µ—Ä–µ—á–∏—Å–ª–∏|—Å–ø–∏—Å–æ–∫|–≤—Å–µ)\s+(—É—Ä–æ–∫–∏|—É—Ä–æ–∫–æ–≤)\b",
    re.IGNORECASE,
)
MODULE_NUM_RE = re.compile(r"\b–º–æ–¥—É–ª[—å—è–µ—é]\s*(\d+)\b", re.IGNORECASE)

def normalize(s: str) -> str:
    s = (s or "").lower().strip().replace("—ë", "–µ")
    s = re.sub(r"[\"'‚Äú‚Äù¬´¬ª]", "", s)
    s = re.sub(r"\s+", " ", s)
    return s

def should_show_kb_links(text: str) -> bool:
    if not text:
        return False

    t = normalize(text)

    has_lesson_word = ("—É—Ä–æ–∫" in t)
    has_locator_words = any(w in t for w in [
        "–≥–¥–µ", "–≤ –∫–∞–∫–æ–º", "–∫–∞–∫–æ–π", "–ª–µ–∂–∏—Ç", "–Ω–∞–π—Ç–∏", "–æ—Ç–∫—Ä—ã—Ç—å", "–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å",
        "–≤ –∫—É—Ä—Å–µ", "–≤ –æ–±—É—á–µ–Ω–∏–∏", "–≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ", "–≤ –º–æ–¥—É–ª–µ", "–≤ —Å—Ç—É–ø–µ–Ω–∏",
        "–º–æ–¥—É–ª—å", "—Å—Ç—É–ø–µ–Ω—å", "—Ä–∞–∑–¥–µ–ª"
    ])

    if has_lesson_word and has_locator_words:
        return True

    return bool(COURSE_LOCATOR_RE.search(text))

def is_howto(text: str) -> bool:
    return bool(text and HOWTO_RE.search(text))

def wants_list_lessons(text: str) -> bool:
    return bool(text and LIST_LESSONS_RE.search(text))

def extract_module_num(text: str) -> Optional[int]:
    if not text:
        return None
    m = MODULE_NUM_RE.search(text)
    if not m:
        return None
    try:
        return int(m.group(1))
    except Exception:
        return None

# ================= KB (parse + semantic retrieval + LLM rerank) =================
KB_INDEX: List[Dict[str, Any]] = []
KB_EMB_VECS: List[List[float]] = []

COURSE_RE = re.compile(r"^\s*–°–¢–†–£–ö–¢–£–†–ê\s+–ö–£–†–°–ê", re.IGNORECASE)
STEP_RE = re.compile(r"^\s*\d+\s*—Å—Ç—É–ø–µ–Ω", re.IGNORECASE)
MODULE_RE = re.compile(r"^\s*\d+\s*–º–æ–¥—É–ª", re.IGNORECASE)
LESSON_RE = re.compile(r"^\s*\d+\s*—É—Ä–æ–∫\b", re.IGNORECASE)

URL_LINE_RE = re.compile(r"(https?://\S+)", re.IGNORECASE)
LESSON_URL_LINE_RE = re.compile(r"(?:–°—Å—ã–ª–∫–∞\s+–Ω–∞\s+—É—Ä–æ–∫|–°—Å—ã–ª–∫–∞)\s*:\s*(https?://\S+)", re.IGNORECASE)
DZ_RE = re.compile(r"^\s*–î–ó(?:\s*\([^)]+\))?\s*:\s*(.+)$", re.IGNORECASE)

SECTION_FULL_RE = re.compile(
    r"^–†–∞–∑–¥–µ–ª\s*[¬´\"\']?([^¬ª\"\'\:]+)[¬ª\"\']?\s*:\s*(.+)$",
    re.IGNORECASE
)

def split_materials(s: str) -> List[str]:
    parts = [p.strip() for p in (s or "").split(",")]
    parts = [p.strip(" .;") for p in parts if p.strip()]
    return parts

def load_kb() -> Tuple[int, str]:
    global KB_INDEX
    if not os.path.exists(KB_PATH):
        KB_INDEX = []
        return 0, f"KB not found: {KB_PATH}"

    lines = [ln.rstrip() for ln in open(KB_PATH, "r", encoding="utf-8", errors="ignore").read().splitlines()]
    if not lines:
        KB_INDEX = []
        return 0, "KB empty"

    course_title, course_url = "", ""
    step_title, step_url = "", ""
    module_title, module_url = "", ""

    KB_INDEX = []
    i = 0
    while i < len(lines):
        ln = (lines[i] or "").strip()
        if not ln:
            i += 1
            continue

        if COURSE_RE.search(ln):
            course_title = ln
            course_url = ""
            j = i + 1
            while j < len(lines) and not lines[j].strip():
                j += 1
            if j < len(lines):
                m = URL_LINE_RE.search(lines[j].strip())
                if m:
                    course_url = m.group(1).rstrip(").,;")
                    i = j
            i += 1
            continue

        if STEP_RE.search(ln):
            step_title = ln
            step_url = ""
            j = i + 1
            while j < len(lines) and not lines[j].strip():
                j += 1
            if j < len(lines):
                m = URL_LINE_RE.search(lines[j].strip())
                if m:
                    step_url = m.group(1).rstrip(").,;")
                    i = j
            module_title, module_url = "", ""
            i += 1
            continue

        if MODULE_RE.search(ln):
            module_title = ln
            module_url = ""
            j = i + 1
            while j < len(lines) and not lines[j].strip():
                j += 1
            if j < len(lines):
                m = URL_LINE_RE.search(lines[j].strip())
                if m:
                    module_url = m.group(1).rstrip(").,;")
                    i = j
            i += 1
            continue

        if LESSON_RE.search(ln):
            lesson_title = ln
            lesson_url = ""
            sections: List[str] = []
            homework = ""

            j = i + 1
            while j < len(lines):
                cur = (lines[j] or "").strip()
                if not cur:
                    j += 1
                    continue
                if COURSE_RE.search(cur) or STEP_RE.search(cur) or MODULE_RE.search(cur) or LESSON_RE.search(cur):
                    break

                mlu = LESSON_URL_LINE_RE.search(cur)
                if mlu:
                    lesson_url = mlu.group(1).rstrip(").,;")
                else:
                    mdz = DZ_RE.search(cur)
                    if mdz:
                        homework = mdz.group(1).strip()
                    else:
                        sections.append(cur)

                j += 1

            lesson_blob = "\n".join(sections).strip()

            section_items: List[Tuple[str, List[str]]] = []
            for s in sections:
                msec = SECTION_FULL_RE.match(s.strip())
                if not msec:
                    continue
                sec_title = msec.group(1).strip()
                materials_raw = msec.group(2).strip()
                mats = split_materials(materials_raw) if materials_raw else []
                section_items.append((sec_title, mats))

            if not section_items:
                section_items = [("", ["(–º–∞—Ç–µ—Ä–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω)"])]

            for sec_title, mats in section_items:
                if not mats:
                    mats = ["(–º–∞—Ç–µ—Ä–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω)"]

                for mat in mats:
                    KB_INDEX.append({
                        "type": "micro",
                        "kind": "–í–∏–¥–µ–æ—É—Ä–æ–∫",
                        "course_title": course_title,
                        "course_url": course_url,
                        "step_title": step_title,
                        "step_url": step_url,
                        "module_title": module_title,
                        "module_url": module_url,
                        "lesson_title": lesson_title,
                        "lesson_url": lesson_url,
                        "section_title": sec_title,
                        "material_title": mat,
                        "homework": homework,
                        "lesson_blob": lesson_blob,
                        "text": normalize(" ".join([
                            course_title, step_title, module_title, lesson_title,
                            sec_title, mat, lesson_blob, homework
                        ])),
                    })

            i = j
            continue

        i += 1

    return len(KB_INDEX), "OK"

def _cosine(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return -1.0
    dot = 0.0
    na = 0.0
    nb = 0.0
    for i in range(len(a)):
        x = float(a[i])
        y = float(b[i])
        dot += x * y
        na += x * x
        nb += y * y
    if na <= 0 or nb <= 0:
        return -1.0
    return dot / (math.sqrt(na) * math.sqrt(nb))

def load_embeddings() -> Tuple[int, str]:
    global KB_EMB_VECS
    KB_EMB_VECS = []

    if not os.path.exists(KB_EMB_PATH):
        return 0, f"Embeddings not found: {KB_EMB_PATH}"

    try:
        data = json.loads(open(KB_EMB_PATH, "r", encoding="utf-8", errors="ignore").read())
    except Exception as e:
        return 0, f"Embeddings JSON read error: {repr(e)}"

    items = None
    if isinstance(data, list):
        items = data
    elif isinstance(data, dict):
        if isinstance(data.get("items"), list):
            items = data["items"]
        elif isinstance(data.get("data"), list):
            items = data["data"]
        elif isinstance(data.get("embeddings"), list):
            items = data["embeddings"]

    if not isinstance(items, list):
        return 0, "Embeddings JSON format not recognized"

    vecs: List[List[float]] = []
    for it in items:
        if isinstance(it, dict):
            emb = it.get("embedding")
            if isinstance(emb, list) and emb and isinstance(emb[0], (int, float)):
                vecs.append([float(x) for x in emb])

    if not vecs:
        return 0, "No vectors found in embeddings.json"

    if KB_INDEX and len(vecs) >= len(KB_INDEX):
        KB_EMB_VECS = vecs[:len(KB_INDEX)]
        return len(KB_EMB_VECS), "OK (aligned)"
    else:
        KB_EMB_VECS = vecs
        return len(KB_EMB_VECS), "OK (unaligned)"

def _expand_query_for_semantic(q: str) -> str:
    t = normalize(q)
    add: List[str] = []

    if "–º–∏–¥" in t or "mid" in t:
        add += ["–º–∏–¥-—Å–µ–Ω—á—É—Ä–∏", "mid-century", "mid century", "midcentury", "–º–∏–¥—Å–µ–Ω—á—É—Ä–∏"]
    if "—ç–∫–æ" in t or "eco" in t:
        add += ["—ç–∫–æ-—Å—Ç–∏–ª—å", "eco style", "—ç–∫–æ—Å—Ç–∏–ª—å"]
    if "—Å—Ä–µ–¥–∏–∑–µ–º" in t or "mediterr" in t:
        add += ["—Å—Ä–µ–¥–∏–∑–µ–º–Ω–æ–º–æ—Ä—Å–∫–∏–π", "mediterranean"]
    if "–º–µ–º—Ñ–∏—Å" in t or "memphis" in t:
        add += ["–º–µ–º—Ñ–∏—Å", "memphis"]

    if "–≤–∞–Ω–Ω" in t or "bath" in t:
        add += ["—Å–∞–Ω—É–∑–µ–ª", "—Å–∞–Ω—É–∑–ª—ã", "—Ç—É–∞–ª–µ—Ç", "bathroom", "wc"]
    if "—Å–∞–Ω—É–∑" in t or "—Ç—É–∞–ª–µ—Ç" in t or "wc" in t:
        add += ["–≤–∞–Ω–Ω–∞—è", "–≤–∞–Ω–Ω–∞", "bathroom"]

    if "—Ñ–æ—Ç–æ—à–æ–ø" in t or "photoshop" in t or "ps" in t:
        add += ["adobe photoshop", "—Ñ—à", "psd"]
    if "3–¥" in t or "3d" in t:
        add += ["3d", "3–¥", "3–¥ –∫–æ–ª–ª–∞–∂", "3d collage", "–∫–æ–ª–ª–∞–∂", "moodboard", "–º—É–¥–±–æ—Ä–¥"]

    if add:
        return (q + "\n\n–°–∏–Ω–æ–Ω–∏–º—ã/–ø–µ—Ä–µ–≤–æ–¥—ã: " + ", ".join(add)).strip()
    return q

def kb_candidates_semantic(query: str, k: int = 20) -> List[Dict[str, Any]]:
    if not query or not KB_INDEX or not KB_EMB_VECS:
        return []

    q2 = _expand_query_for_semantic(query)
    qvec = openai_embed(q2)
    if not qvec:
        return []

    n = min(len(KB_INDEX), len(KB_EMB_VECS))
    if n <= 0:
        return []

    scored: List[Tuple[float, int]] = []
    for i in range(n):
        sim = _cosine(qvec, KB_EMB_VECS[i])
        if sim > -0.5:
            scored.append((sim, i))

    scored.sort(key=lambda x: x[0], reverse=True)

    out: List[Dict[str, Any]] = []
    seen = set()
    for sim, idx in scored[: max(k * 4, 40)]:
        it = KB_INDEX[idx]
        key = (it.get("lesson_url"), it.get("section_title"), it.get("material_title"))
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
        if len(out) >= k:
            break
    return out

def kb_candidates_keyword(query: str, k: int = 20) -> List[Dict[str, Any]]:
    if not query or not KB_INDEX:
        return []

    q = normalize(query)

    expansions: List[str] = []
    if "–≤–∞–Ω–Ω" in q:
        expansions += ["—Å–∞–Ω—É–∑–µ–ª", "—Å–∞–Ω—É–∑–ª—ã", "—Ç—É–∞–ª–µ—Ç", "bathroom", "wc"]
    if "—Å–∞–Ω—É–∑" in q or "—Ç—É–∞–ª–µ—Ç" in q or "wc" in q:
        expansions += ["–≤–∞–Ω–Ω–∞—è", "–≤–∞–Ω–Ω–∞", "bathroom"]

    if "3d" in q or "3–¥" in q:
        expansions += ["3–¥", "3d", "–∫–æ–ª–ª–∞–∂", "3–¥ –∫–æ–ª–ª–∞–∂", "3d collage", "–º—É–¥–±–æ—Ä–¥", "moodboard"]
    if "–∫–æ–ª–ª–∞–∂" in q or "moodboard" in q:
        expansions += ["3–¥", "3d", "3–¥ –∫–æ–ª–ª–∞–∂", "3d collage"]

    if "photoshop" in q or "—Ñ–æ—Ç–æ—à–æ–ø" in q:
        expansions += ["ps", "adobe photoshop"]

    if "–º–∏–¥" in q or "mid" in q:
        expansions += ["–º–∏–¥-—Å–µ–Ω—á—É—Ä–∏", "mid-century", "mid century", "–º–∏–¥—Å–µ–Ω—á—É—Ä–∏"]
    if "—ç–∫–æ" in q or "eco" in q:
        expansions += ["—ç–∫–æ-—Å—Ç–∏–ª—å", "eco style", "—ç–∫–æ—Å—Ç–∏–ª—å"]

    terms = [w for w in re.findall(r"[a-z–∞-—è0-9]+", q) if len(w) >= 3]
    for e in expansions:
        terms += [w for w in re.findall(r"[a-z–∞-—è0-9]+", normalize(e)) if len(w) >= 3]
    terms = list(dict.fromkeys(terms))

    need_bath = ("–≤–∞–Ω–Ω" in q) or ("—Å–∞–Ω—É–∑" in q) or ("—Ç—É–∞–ª–µ—Ç" in q) or ("bath" in q) or ("wc" in q)
    bath_terms = ["—Å–∞–Ω—É–∑", "–≤–∞–Ω–Ω", "—Ç—É–∞–ª–µ—Ç", "bath", "wc"]

    scored: List[Tuple[float, Dict[str, Any]]] = []
    for it in KB_INDEX:
        t = it.get("text", "")
        if not t:
            continue

        score = 0.0
        mt = normalize(it.get("material_title", ""))
        lb = normalize(it.get("lesson_blob", ""))
        hw = normalize(it.get("homework", ""))

        for w in terms:
            if w in t:
                score += 1.0
            if w in mt:
                score += 1.5
            if w in lb:
                score += 1.2
            if w in hw:
                score += 2.6

        if need_bath and any(bt in t for bt in bath_terms):
            score += 3.0

        if score > 0:
            scored.append((score, it))

    scored.sort(key=lambda x: x[0], reverse=True)

    uniq: List[Dict[str, Any]] = []
    seen = set()
    for _, it in scored:
        key = (it.get("lesson_url"), it.get("section_title"), it.get("material_title"))
        if key in seen:
            continue
        seen.add(key)
        uniq.append(it)
        if len(uniq) >= k:
            break

    return uniq

def kb_candidates(query: str, k: int = 20) -> List[Dict[str, Any]]:
    sem = kb_candidates_semantic(query, k=k)
    if sem:
        return sem
    return kb_candidates_keyword(query, k=k)

def kb_select_with_llm(user_query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not candidates:
        return []

    packed = []
    for idx, it in enumerate(candidates[:20], 1):
        packed.append({
            "id": idx,
            "step": it.get("step_title", ""),
            "module": it.get("module_title", ""),
            "lesson": it.get("lesson_title", ""),
            "section": it.get("section_title", ""),
            "material": it.get("material_title", ""),
            "url": it.get("lesson_url", ""),
            "homework": it.get("homework", ""),
            "blob": (it.get("lesson_blob", "")[:550] if it.get("lesson_blob") else ""),
        })

    selector_system = (
        "–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –∏ –∫—É—Ä–∞—Ç–æ—Ä –∫—É—Ä—Å–∞ –¥–∏–∑–∞–π–Ω–∞ –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞.\n"
        "–¢–µ–±–µ –¥–∞–ª–∏ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞ –∏ —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.\n"
        "–ó–∞–¥–∞—á–∞: –≤—ã–±—Ä–∞—Ç—å –¥–æ 3 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ü–û –°–ú–´–°–õ–£.\n"
        "–£—á–∏—Ç—ã–≤–∞–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ–≤–æ–¥—ã RU<->EN (mid-century = –º–∏–¥-—Å–µ–Ω—á—É—Ä–∏), —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤ –î–ó, —Ü–µ–ª—å —É—Ä–æ–∫–∞.\n"
        "–ï—Å–ª–∏ –≤ –±–∞–∑–µ –ù–ï–¢ –Ω–∏—á–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ ‚Äî –≤–µ—Ä–Ω–∏ NONE.\n"
        "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥:\n"
        "{\"pick\":[1,5],\"reason\":\"...\"} –∏–ª–∏ {\"pick\":[],\"reason\":\"NONE\"}\n"
    )

    raw = openai_chat(
        [
            {"role": "system", "content": selector_system},
            {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞:\n{user_query}\n\n–ö–∞–Ω–¥–∏–¥–∞—Ç—ã:\n{json.dumps(packed, ensure_ascii=False)}"},
        ],
        max_tokens=350,
        temperature=0.2,
    )

    try:
        data = json.loads(raw)
        picks = data.get("pick", [])
        if not picks:
            return []
        chosen = []
        for p in picks[:3]:
            if isinstance(p, int) and 1 <= p <= len(candidates[:20]):
                chosen.append(candidates[p - 1])
        return chosen
    except Exception:
        return candidates[:1]

def best_material_name(it: Dict[str, Any], user_query: str) -> str:
    q = normalize(user_query)
    hw = (it.get("homework") or "").strip()
    mat = (it.get("material_title") or "").strip()

    if ("–∫–æ–ª–ª–∞–∂" in q or "3–¥" in q or "3d" in q) and hw:
        return hw if len(hw) <= 240 else (hw[:240].rstrip() + "‚Ä¶")

    return mat or "(–º–∞—Ç–µ—Ä–∏–∞–ª)"

def dedupe_hits_by_lesson(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not hits:
        return []
    out: List[Dict[str, Any]] = []
    seen = set()
    for it in hits:
        key = (it.get("lesson_url") or "").strip() or (it.get("lesson_title") or "").strip()
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def format_kb_hits(hits: List[Dict[str, Any]], user_query: str) -> str:
    if not hits:
        return ""

    hits = dedupe_hits_by_lesson(hits)

    out = ["\n\nüìö <b>–ù–∞—à–ª–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏:</b>"]
    for it in hits[:3]:
        out.append("\nüìå <b>–ú–∞—Ç–µ—Ä–∏–∞–ª:</b> –í–∏–¥–µ–æ—É—Ä–æ–∫")

        out.append("\n<b>–ì–¥–µ –ª–µ–∂–∏—Ç –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ:</b>")
        if it.get("step_title"):
            out.append(f"‚Äî <b>–°—Ç—É–ø–µ–Ω—å:</b> {it.get('step_title')}")
        if it.get("module_title"):
            out.append(f"‚Äî <b>–ú–æ–¥—É–ª—å:</b> {it.get('module_title')}")
        if it.get("lesson_title"):
            out.append(f"‚Äî <b>–£—Ä–æ–∫:</b> {it.get('lesson_title')}")
        if it.get("section_title"):
            out.append(f"‚Äî <b>–†–∞–∑–¥–µ–ª:</b> {it.get('section_title')}")

        out.append(f"\n<b>–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞:</b> {best_material_name(it, user_query)}")

        url = it.get("lesson_url", "")
        if url:
            out.append(f"\n<b>–°—Å—ã–ª–∫–∞:</b>\n{url}")

        hw = (it.get("homework") or "").strip()
        if hw:
            out.append("\n<b>–î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ:</b>")
            out.append(f"{hw}")

    return "\n".join(out).strip()

def format_module_lessons(module_num: int) -> str:
    if not KB_INDEX:
        return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ üôè"

    target = str(module_num)
    module_hits: List[Dict[str, Any]] = []
    for it in KB_INDEX:
        mt = (it.get("module_title") or "").strip().lower().replace("—ë", "–µ")
        if re.search(rf"(^|\s){re.escape(target)}\s*–º–æ–¥—É–ª", mt):
            module_hits.append(it)

    if not module_hits:
        return f"–ù–µ –Ω–∞—à–ª–∞ –º–æ–¥—É–ª—å {module_num} –≤ –±–∞–∑–µ üôè –ï—Å–ª–∏ —Å–∫–∞–∂–µ—à—å —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è ‚Äî –Ω–∞–π–¥—É —Ç–æ—á–Ω–µ–µ."

    lessons: Dict[str, Dict[str, str]] = {}
    for it in module_hits:
        lt = (it.get("lesson_title") or "").strip()
        url = (it.get("lesson_url") or "").strip()
        if not lt:
            continue
        if lt not in lessons:
            lessons[lt] = {"url": url}
        if not lessons[lt]["url"] and url:
            lessons[lt]["url"] = url

    if not lessons:
        return f"–í –º–æ–¥—É–ª–µ {module_num} –Ω–∞—à–ª–∏—Å—å –∑–∞–ø–∏—Å–∏, –Ω–æ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏–π —É—Ä–æ–∫–æ–≤. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç knowledge.txt üôè"

    def lesson_sort_key(title: str) -> Tuple[int, str]:
        m = re.search(r"(\d+)\s*—É—Ä–æ–∫", title.lower())
        if m:
            return (int(m.group(1)), title)
        return (10**9, title)

    ordered = sorted(lessons.items(), key=lambda kv: lesson_sort_key(kv[0]))

    out: List[str] = []
    out.append(f"üìö <b>–ú–æ–¥—É–ª—å {module_num}: —É—Ä–æ–∫–∏</b>\n")
    for title, meta in ordered:
        url = (meta.get("url") or "").strip()
        if url:
            out.append(f"‚Äî <b>{title}</b>\n{url}")
        else:
            out.append(f"‚Äî <b>{title}</b>\n(—Å—Å—ã–ª–∫–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞ –≤ –±–∞–∑–µ)")

    out.append("\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî —É—Ç–æ—á–Ω–∏ —Ç–µ–º—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´3D –∫–æ–ª–ª–∞–∂ –≤–∞–Ω–Ω–æ–π¬ª), –∏ —è –¥–∞–º —Ç–æ—á–Ω—ã–π —É—Ä–æ–∫ –∏ —Ä–∞–∑–¥–µ–ª.")
    return "\n".join(out).strip()

@app.on_event("startup")
def _startup():
    n, msg = load_kb()
    print(f"KB loaded: {n} items, status: {msg}")

    en, emsg = load_embeddings()
    print(f"Embeddings loaded: {en} vectors, status: {emsg}")

# ================= ALBUM PROCESSOR =================
async def _process_album(chat_id: int, album_id: str):
    key = (chat_id, album_id)
    data = ALBUM_BUFFER.get(key)
    if not data:
        return

    await asyncio.sleep(ALBUM_DEBOUNCE_SEC)

    data2 = ALBUM_BUFFER.get(key)
    if not data2 or data2.get("task_id") != data.get("task_id"):
        return

    images: List[bytes] = data2.get("images", [])
    caption: str = (data2.get("caption") or "").strip()

    ALBUM_BUFFER.pop(key, None)

    if not images:
        return

    tg_typing(chat_id)

    nums: List[int] = []
    descs: List[Tuple[int, str]] = []
    for idx, img in enumerate(images, 1):
        describe_prompt = (
            "–û–ø–∏—à–∏, —á—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —á—Ç–æ–±—ã —è –º–æ–≥–ª–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö. "
            "–ï—Å–ª–∏ —ç—Ç–æ –∏–Ω—Ç–µ—Ä—å–µ—Ä ‚Äî —É–∫–∞–∂–∏ —Ç–∏–ø –ø–æ–º–µ—â–µ–Ω–∏—è, –ø–ª–∞–Ω, –º–µ–±–µ–ª—å, —Ü–≤–µ—Ç–∞, —Å–≤–µ—Ç, —Å—Ç–∏–ª—å. "
            "–ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç ‚Äî —É–∫–∞–∂–∏ –º–∞—Ç–µ—Ä–∏–∞–ª, —Ü–≤–µ—Ç, —Ñ–æ—Ä–º—É, —Å—Ç–∏–ª—å, —Ñ–∞–∫—Ç—É—Ä—É. "
            "–°–¥–µ–ª–∞–π 5‚Äì7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –±–µ–∑ –≤–æ–¥—ã."
        )
        desc = openai_with_image(describe_prompt, img, [], max_tokens=320, temperature=0.35)
        num = push_image(chat_id, img, desc, album_id=album_id)
        nums.append(num)
        descs.append((num, desc))

    add_context(chat_id, "user", f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –∞–ª—å–±–æ–º —Ñ–æ—Ç–æ: {', '.join([f'#{n}' for n in nums])}]")

    if caption:
        add_context(chat_id, "user", caption)

        if COMPARE_RE.search(caption) and len(descs) >= 2:
            packed = "\n\n".join([f"–§–æ—Ç–æ #{n}:\n{d}" for n, d in descs])
            messages = [
                {"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)},
                {
                    "role": "user",
                    "content": (
                        f"{caption}\n\n"
                        "–ù–∏–∂–µ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (—ç—Ç–æ –æ–¥–∏–Ω –∞–ª—å–±–æ–º). "
                        "–í—ã–±–µ—Ä–∏ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∏ –æ–±—ä—è—Å–Ω–∏:\n"
                        "1) –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: —Ñ–æ—Ç–æ #...\n"
                        "2) 3 –ø—Ä–∏—á–∏–Ω—ã\n"
                        "3) –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–∏–≥—Ä–∞–≤—à–µ–≥–æ ‚Äî –ø–æ 1 —Ç–æ—á–µ—á–Ω–æ–π –ø—Ä–∞–≤–∫–µ (–∫–æ—Ä–æ—Ç–∫–æ)\n"
                        "–ë–µ–∑ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑.\n\n"
                        f"{packed}"
                    ),
                },
            ]
            answer = openai_chat(messages, max_tokens=650, temperature=0.55)
            remember_assistant(chat_id, answer)
            add_context(chat_id, "assistant", answer)
            tg_send(chat_id, answer)
            return

        packed = "\n\n".join([f"–§–æ—Ç–æ #{n}:\n{d}" for n, d in descs])
        messages = [
            {"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)},
            {
                "role": "user",
                "content": (
                    f"{caption}\n\n"
                    "–£ —Ç–µ–±—è –µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –≤—Å–µ—Ö —Ñ–æ—Ç–æ –∏–∑ –∞–ª—å–±–æ–º–∞ –Ω–∏–∂–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
                    "–û—Ç–≤–µ—á–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É, –±–µ–∑ —Ñ—Ä–∞–∑—ã ¬´—è –Ω–µ –≤–∏–∂—É —Ñ–æ—Ç–æ¬ª. "
                    "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –≤—ã–±–æ—Ä ‚Äî –≤—ã–±–µ—Ä–∏ –∏ –æ–±–æ—Å–Ω—É–π.\n\n"
                    f"{packed}"
                ),
            },
        ]
        answer = openai_chat(messages, max_tokens=750, temperature=0.5)
        remember_assistant(chat_id, answer)
        add_context(chat_id, "assistant", answer)
        tg_send(chat_id, answer)
        return

    focus = ["–∫–æ–º–ø–æ–∑–∏—Ü–∏—é", "—Å–≤–µ—Ç", "—Ü–≤–µ—Ç", "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ —Ñ–∞–∫—Ç—É—Ä—ã"]
    f = focus[nums[-1] % len(focus)]
    packed_short = "\n\n".join([f"–§–æ—Ç–æ #{n}: {d[:220].strip()}‚Ä¶" for n, d in descs])
    messages = [
        {"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)},
        {
            "role": "user",
            "content": (
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –∞–ª—å–±–æ–º –∏–∑ {len(descs)} —Ñ–æ—Ç–æ –±–µ–∑ –ø–æ–¥–ø–∏—Å–∏.\n"
                f"–î–∞–π –æ–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (3‚Äì5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ {f}. "
                "–ó–∞—Ç–µ–º –∑–∞–¥–∞–π –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ —á–µ–ª–æ–≤–µ–∫—É –Ω—É–∂–Ω–æ: –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç, –Ω–∞–π—Ç–∏ –æ—à–∏–±–∫–∏, "
                "–∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø—Ä–∞–≤–∫–∏. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —à—Ç–∞–º–ø—ã.\n\n"
                f"–û–ø–∏—Å–∞–Ω–∏—è —Ñ–æ—Ç–æ:\n{packed_short}"
            ),
        },
    ]
    answer = openai_chat(messages, max_tokens=320, temperature=0.6)
    remember_assistant(chat_id, answer)
    add_context(chat_id, "assistant", answer)
    tg_send(chat_id, answer)

def _schedule_album(chat_id: int, album_id: str):
    key = (chat_id, album_id)
    data = ALBUM_BUFFER.get(key)
    if not data:
        return
    data["task_id"] = str(time.time())
    ALBUM_BUFFER[key] = data
    asyncio.create_task(_process_album(chat_id, album_id))

# ================= WEBHOOK =================
@app.post("/webhook")
async def webhook(req: Request):
    update = await req.json()
    msg = update.get("message")
    if not msg:
        return {"ok": True}

    chat_id = msg["chat"]["id"]
    text = (msg.get("text") or "").strip()
    caption = (msg.get("caption") or "").strip()
    photos = msg.get("photo") or []
    album_id = msg.get("media_group_id")

    # ===== PHOTO RECEIVED =====
    if photos:
        img = tg_get_photo(photos[-1]["file_id"])
        if not img:
            return {"ok": True}

        LAST_IMAGE[chat_id] = img
        LAST_IMAGE_AT[chat_id] = time.time()

        # ---- ALBUM MODE (Variant 2): buffer and answer once ----
        if album_id:
            key = (chat_id, str(album_id))
            buf = ALBUM_BUFFER.get(key) or {"images": [], "caption": "", "task_id": ""}
            buf["images"].append(img)
            if caption and not buf.get("caption"):
                buf["caption"] = caption
            ALBUM_BUFFER[key] = buf
            _schedule_album(chat_id, str(album_id))
            return {"ok": True}

        # ---- SINGLE PHOTO (existing behavior) ----
        tg_typing(chat_id)

        describe_prompt = (
            "–û–ø–∏—à–∏, —á—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —á—Ç–æ–±—ã —è –º–æ–≥–ª–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö. "
            "–ï—Å–ª–∏ —ç—Ç–æ –∏–Ω—Ç–µ—Ä—å–µ—Ä ‚Äî —É–∫–∞–∂–∏ —Ç–∏–ø –ø–æ–º–µ—â–µ–Ω–∏—è, –ø–ª–∞–Ω, –º–µ–±–µ–ª—å, —Ü–≤–µ—Ç–∞, —Å–≤–µ—Ç, —Å—Ç–∏–ª—å. "
            "–ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç ‚Äî —É–∫–∞–∂–∏ –º–∞—Ç–µ—Ä–∏–∞–ª, —Ü–≤–µ—Ç, —Ñ–æ—Ä–º—É, —Å—Ç–∏–ª—å, —Ñ–∞–∫—Ç—É—Ä—É. "
            "–°–¥–µ–ª–∞–π 6‚Äì8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –±–µ–∑ –≤–æ–¥—ã."
        )
        visual_description = openai_with_image(describe_prompt, img, [], max_tokens=350, temperature=0.35)
        num = push_image(chat_id, img, visual_description)

        # If caption exists -> answer it with vision
        if caption:
            add_context(chat_id, "user", caption)
            ctx = get_context(chat_id)
            answer = openai_with_image(caption, img, ctx, max_tokens=900, temperature=0.55)
            remember_assistant(chat_id, answer)
            add_context(chat_id, "assistant", answer)
            tg_send(chat_id, answer)
            return {"ok": True}

        # No caption -> auto comment
        add_context(chat_id, "user", f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª —Ñ–æ—Ç–æ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞. –§–æ—Ç–æ #{num}]")
        auto_prompt = (
            "–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ç—ë–ø–ª—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ —Ç–æ–º—É, —á—Ç–æ –Ω–∞ —Ñ–æ—Ç–æ: "
            "2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –±–µ–∑ —à—Ç–∞–º–ø–æ–≤; –æ–¥–Ω–æ —Å–∏–ª—å–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ + –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ."
        )
        auto_answer = openai_with_image(auto_prompt, img, [], max_tokens=220, temperature=0.6)
        remember_assistant(chat_id, auto_answer)
        add_context(chat_id, "assistant", auto_answer)
        tg_send(chat_id, auto_answer)
        return {"ok": True}

    # ===== TEXT MESSAGE =====
    if text:
        tg_typing(chat_id)

        # ===== DAILY LIMIT CHECK =====
        ok, remaining = can_reply_today(chat_id)
        if not ok:
            tg_send(
                chat_id,
                "–ú—ã —Å–µ–≥–æ–¥–Ω—è —É–∂–µ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏ üíõ\n"
                "–Ø –æ—Ç–≤–µ—á–∞—é –ø–æ–¥—Ä–æ–±–Ω–æ, –ø–æ—ç—Ç–æ–º—É –µ—Å—Ç—å –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç.\n\n"
                "–ó–∞–≤—Ç—Ä–∞ –ø—Ä–æ–¥–æ–ª–∂–∏–º ‚Äî –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å—Ä–æ—á–Ω—ã–π, –ø–æ–ø—Ä–æ–±—É–π —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º."
            )
            inc_today(chat_id)
            return {"ok": True}

        # ===== TOPIC GUARD (forbidden topics) =====
        if is_forbidden_topic(text):
            tg_send(chat_id, OFFTOP_REPLY)
            inc_today(chat_id)
            return {"ok": True}

        add_context(chat_id, "user", text)

        # ====== LIST LESSONS FOR MODULE (only if user asked list) ======
        if wants_list_lessons(text):
            mn = extract_module_num(text)
            if mn is not None:
                answer = format_module_lessons(mn)
                remember_assistant(chat_id, answer)
                add_context(chat_id, "assistant", answer)
                tg_send(chat_id, answer)
                inc_today(chat_id)
                return {"ok": True}
            else:
                answer = "–û–∫ üôÇ –ù–∞–ø–∏—à–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: ¬´–≤—Å–µ —É—Ä–æ–∫–∏ –º–æ–¥—É–ª—è 2¬ª (—Å –Ω–æ–º–µ—Ä–æ–º –º–æ–¥—É–ª—è)."
                remember_assistant(chat_id, answer)
                add_context(chat_id, "assistant", answer)
                tg_send(chat_id, answer)
                inc_today(chat_id)
                return {"ok": True}

        # ====== üî• COMPARISON REQUEST (by #numbers or last 2) ======
        if COMPARE_RE.search(text) and len(IMAGE_HISTORY.get(chat_id, [])) >= 2:
            hist = IMAGE_HISTORY.get(chat_id, [])
            nums = [int(x) for x in PHOTO_NUM_RE.findall(text)]

            def get_by_num(n: int) -> Optional[Dict[str, Any]]:
                for it in hist:
                    if it.get("num") == n:
                        return it
                return None

            if len(nums) >= 2:
                a = get_by_num(nums[0])
                b = get_by_num(nums[1])
                if a and b and a.get("image") and b.get("image"):
                    img_a, img_b = a["image"], b["image"]
                    label_a, label_b = f"–§–æ—Ç–æ #{nums[0]}", f"–§–æ—Ç–æ #{nums[1]}"
                else:
                    img_a, img_b = hist[-2]["image"], hist[-1]["image"]
                    label_a, label_b = "–§–æ—Ç–æ A (–ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–µ)", "–§–æ—Ç–æ B (–ø–æ—Å–ª–µ–¥–Ω–µ–µ)"
            else:
                img_a, img_b = hist[-2]["image"], hist[-1]["image"]
                label_a, label_b = "–§–æ—Ç–æ A (–ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–µ)", "–§–æ—Ç–æ B (–ø–æ—Å–ª–µ–¥–Ω–µ–µ)"

            desc_a = openai_with_image(
                f"–ö–æ—Ä–æ—Ç–∫–æ –æ–ø–∏—à–∏ {label_a} –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤: –∫–æ–º–ø–æ–∑–∏—Ü–∏—è, —Ü–≤–µ—Ç, —Å–≤–µ—Ç, —Å—Ç–∏–ª—å, —Ñ—É–Ω–∫—Ü–∏—è).",
                img_a, [], max_tokens=220, temperature=0.35
            )
            desc_b = openai_with_image(
                f"–ö–æ—Ä–æ—Ç–∫–æ –æ–ø–∏—à–∏ {label_b} –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤: –∫–æ–º–ø–æ–∑–∏—Ü–∏—è, —Ü–≤–µ—Ç, —Å–≤–µ—Ç, —Å—Ç–∏–ª—å, —Ñ—É–Ω–∫—Ü–∏—è).",
                img_b, [], max_tokens=220, temperature=0.35
            )

            messages = [
                {"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)},
                {
                    "role": "user",
                    "content": (
                        f"{text}\n\n"
                        f"{label_a} ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ:\n{desc_a}\n\n"
                        f"{label_b} ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ:\n{desc_b}\n\n"
                        "–û—Ç–≤–µ—Ç –æ—Ñ–æ—Ä–º–∏ —Ç–∞–∫:\n"
                        "1) –í—ã–±–æ—Ä: ...\n"
                        "2) –ü–æ—á–µ–º—É (3 –ø—É–Ω–∫—Ç–∞)\n"
                        "3) –ß—Ç–æ —É–ª—É—á—à–∏—Ç—å –≤ –ø—Ä–æ–∏–≥—Ä–∞–≤—à–µ–º (2 –ø—É–Ω–∫—Ç–∞)\n"
                        "–ë–µ–∑ —à—Ç–∞–º–ø–æ–≤ –∏ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑."
                    ),
                },
            ]
            answer = openai_chat(messages, max_tokens=520, temperature=0.55)
            remember_assistant(chat_id, answer)
            add_context(chat_id, "assistant", answer)
            tg_send(chat_id, answer)
            inc_today(chat_id)
            return {"ok": True}

        # ====== KB LINKS ONLY WHEN ASKED "–≥–¥–µ –≤ –∫—É—Ä—Å–µ / –≤ –∫–∞–∫–æ–º —É—Ä–æ–∫–µ / —Å—Å—ã–ª–∫–∞ –Ω–∞ —É—Ä–æ–∫" ======
        kb_block = ""
        if should_show_kb_links(text):
            cand = kb_candidates(text, k=20)
            picked = kb_select_with_llm(text, cand)
            kb_block = format_kb_hits(picked, text)

        if kb_block:
            guide = openai_chat(
                [
                    {"role": "system", "content": SYSTEM_ROLE},
                    {
                        "role": "user",
                        "content": (
                            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: {text}\n"
                            f"–Ø –Ω–∞—à–ª–∞ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ —Ç–∞–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã:\n{kb_block}\n\n"
                            "–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–¥–≤–æ–¥–∫—É (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –∫–∞–∫ –∫—É—Ä–∞—Ç–æ—Ä –ù–ê–®–ï–ô –ø—Ä–æ–≥—Ä–∞–º–º—ã: "
                            "–±–µ–∑ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ —Ç–∏–ø–∞ ¬´–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫—É—Ä—Å–∞/–ø—Ä–æ–≥—Ä–∞–º–º—ã¬ª. "
                            "–°—Ä–∞–∑—É –Ω–∞–ø—Ä–∞–≤—å —á–µ–ª–æ–≤–µ–∫–∞, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—å –∏ —Å —á–µ–≥–æ –Ω–∞—á–∞—Ç—å. "
                            "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏, –æ–Ω–∏ –Ω–∏–∂–µ."
                        ),
                    },
                ],
                max_tokens=140,
                temperature=0.25,
            )
            answer = (guide.strip() + kb_block).strip()
            remember_assistant(chat_id, answer)
            add_context(chat_id, "assistant", answer)
            tg_send(chat_id, answer)
            inc_today(chat_id)
            return {"ok": True}

        # ====== OTHERWISE: NORMAL ANSWER (WITH VISUAL CONTEXT IF RELEVANT) ======
        ctx = get_context(chat_id)
        messages = [{"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)}] + ctx

        has_any_photo = bool(IMAGE_HISTORY.get(chat_id))
        looks_like_visual_question = bool(IMAGE_REF_RE.search(text) or VISUAL_TOPIC_RE.search(text))

        if is_howto(text):
            messages = [{"role": "system", "content": SYSTEM_ROLE + "\n" + avoid_repetition_hint(chat_id)}] + ctx + [{
                "role": "user",
                "content": (
                    f"{text}\n\n"
                    "–û—Ç–≤–µ—Ç—å –∫–∞–∫ –ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏–π –¥–∏–∑–∞–π–Ω–µ—Ä: –¥–∞–π –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω. "
                    "–ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–π —á–µ–ª–æ–≤–µ–∫–∞ '—Å–º–æ—Ç—Ä–µ—Ç—å —É—Ä–æ–∫–∏' –∏ –Ω–µ –¥–∞–≤–∞–π —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞–ª '–≥–¥–µ –≤ –∫—É—Ä—Å–µ'. "
                    "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown (**–∑–≤—ë–∑–¥–æ—á–∫–∏**). "
                    "–í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å: ¬´–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî —Å–∫–∞–∂–∏ ‚Äú–≥–¥–µ —ç—Ç–æ –≤ –∫—É—Ä—Å–µ‚Äù, –∏ —è –¥–∞–º —Ç–æ—á–Ω—ã–π —É—Ä–æ–∫ –∏ —Å—Å—ã–ª–∫—É.¬ª"
                )
            }]

        if has_any_photo and looks_like_visual_question:
            picked_img = pick_image_from_history(chat_id, text)
            if picked_img and picked_img.get("image"):
                answer = openai_with_image(text, picked_img["image"], ctx, max_tokens=900, temperature=0.55)
            else:
                answer = openai_chat(messages, max_tokens=900, temperature=0.45)
        else:
            answer = openai_chat(messages, max_tokens=900, temperature=0.45)

        remember_assistant(chat_id, answer)
        add_context(chat_id, "assistant", answer)
        tg_send(chat_id, answer)
        inc_today(chat_id)
        return {"ok": True}

    return {"ok": True}
